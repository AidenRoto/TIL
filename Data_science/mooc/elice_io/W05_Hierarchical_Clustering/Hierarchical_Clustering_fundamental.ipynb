{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "## 1. 기본 방식\n",
    "\n",
    "- 요약: 가장 짧은 거리의 데이터들끼리 순차적으로 묶어어나가며 클러스터를 만들고, 최종적으로 가장 적합한 시점의 sub-cluster들을 정한다.\n",
    "- 방법\n",
    "    + (1) 모든 데이터 간의 각각의 거리를 계산한다.(nC2회 계산)\n",
    "    + (2) 가장 가까운 두 데이터 $x_1, x_2$를 하나의 클러스터로 묶는다.\n",
    "    + (3) 클러스터를 새로운 하나의 데이터로 취급한다. 역시 다른 데이터들과 거리를 계산\n",
    "    + (4) 하나의 거대한 클러스터를 이룰 때까지 2와 3을 반복한다.\n",
    "\n",
    "![hc](http://tangibleauditoryinterfaces.de/wp-content/uploads/2010/04/durcheinander-cluster-chart.png)\n",
    "\n",
    "- 클러스터 선택: 결과는 위 이미지처럼 dendrogram으로 나온다. 적절한 지점을 선택해서 이미지의 우측처럼 클러스터를 정한다.\n",
    "- 장점: k-means는 먼저 클러스터의 개수를 정해줘야하지만 이 방식은 bottom-up으로 하기 때문에 사용자가 정해줄 필요 없다.\n",
    "\n",
    "> 위 이미지의 dendrogram을 보면 클러스터를 새롭게 만들어나갈수록 점점 클러스터간 거리가 증가한다. 데이터에 따라 새롭게 만들어진 클러스터와 기존 데이터 사이의 거리가 더 짧아질 수도 있어 보인다. 하지만 그럴 일이 없도록 만든 것이 이 방법론이다.\n",
    "증가하도록 만드는게 계산하기에 편하다.\n",
    "가장 작은걸 선택했으니 나머지는 모두 거리가 클거다. 이렇게 계산해나가는것.\n",
    "만약 더 작은게 생겼다? inversion이라고 그렇게 하면 어렵다.\n",
    "\n",
    "- 클러스터를 만들면 그 클러스터의 거리 계산 기준은 중점으로 하는가? 뒤에 다시 설명할 것.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
