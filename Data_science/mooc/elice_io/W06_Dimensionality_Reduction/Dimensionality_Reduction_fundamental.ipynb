{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "feature가 너무 많으면 계산 비용이 높아지고, 원치 않는 결과가 나올 수 있고, overfitting, 시각화 불가능 등의 나쁜 점이 많다. 적절하게 차원을 축소해서 데이터를 preprocessing하는 것이 필요하다.\n",
    "\n",
    "## 1. Feature selection\n",
    "\n",
    "- variable selection, attribute selection, variable subset selection 이라고도 불린다.\n",
    "- 주로 feature가 매우 많거나, feature에 비해 데이터 샘플이 적을 때 사용한다.\n",
    "- Feature selection을 하는 목적\n",
    "    + 모델을 단순화해서 설명하기 쉽도록 만들어준다.\n",
    "    + 훈련 시간 단축\n",
    "    + 차원의 저주 피하기\n",
    "    + 오버피팅을 방지해서 generalization 강화\n",
    "\n",
    "### 1.1 Wrapper\n",
    "\n",
    "- 확률 모델을 사용한다.\n",
    "- Each new subset is used to train a model, which is tested on a hold-out set. Counting the number of mistakes made on that hold-out set (the error rate of the model) gives the score for that subset. As wrapper methods train a new model for each subset, they are very computationally intensive, but usually provide the best performing feature set for that particular type of model.\n",
    "\n",
    "### 1.2 Filter\n",
    "\n",
    "Filter methods use a proxy measure instead of the error rate to score a feature subset. This measure is chosen to be fast to compute, while still capturing the usefulness of the feature set. Common measures include the mutual information,[3] the pointwise mutual information,[4] Pearson product-moment correlation coefficient, inter/intra class distance or the scores of significance tests for each class/feature combinations.[4][5] Filters are usually less computationally intensive than wrappers, but they produce a feature set which is not tuned to a specific type of predictive model.[6] This lack of tuning means a feature set from a filter is more general than the set from a wrapper, usually giving lower prediction performance than a wrapper. However the feature set doesn't contain the assumptions of a prediction model, and so is more useful for exposing the relationships between the features. Many filters provide a feature ranking rather than an explicit best feature subset, and the cut off point in the ranking is chosen via cross-validation. Filter methods have also been used as a preprocessing step for wrapper methods, allowing a wrapper to be used on larger problems.\n",
    "\n",
    "### 1.3 Embedded\n",
    "\n",
    "Embedded methods are a catch-all group of techniques which perform feature selection as part of the model construction process. The exemplar of this approach is the LASSO method for constructing a linear model, which penalizes the regression coefficients with an L1 penalty, shrinking many of them to zero. Any features which have non-zero regression coefficients are 'selected' by the LASSO algorithm. Improvements to the LASSO include Bolasso which bootstraps samples,[7] and FeaLect which scores all the features based on combinatorial analysis of regression coefficients.[8] One other popular approach is the Recursive Feature Elimination algorithm, commonly used with Support Vector Machines to repeatedly construct a model and remove features with low weights. These approaches tend to be between filters and wrappers in terms of computational complexity.\n",
    "\n",
    "## 2. Feature extraction\n",
    "\n",
    "숨겨진 새로운 feature를 찾아내기. 기존 feature를 조합해서 새로운 feature를 만든다는 의미다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
