{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM, Support Vector Machine\n",
    "\n",
    "## 1. 이론\n",
    "\n",
    "- SVM은 classfier이며 데이터를 잘 구분하는 hyperplane을 찾는 것이 목표다.\n",
    "- hyperplane은 Yes-No를 가르는 기준, 즉 Classifier를 의미한다. P차원의 feature일 때 항상 P-1차원의 함수가 된다.\n",
    "\n",
    "\\begin{align}\n",
    "\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p = 0\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "### 1.1 Maximal Margin Classifier\n",
    "\n",
    "#### 1.1.1 개념\n",
    "\n",
    "![svm](https://lukelushu.files.wordpress.com/2014/09/margin_width1.png)\n",
    "\n",
    "- 데이터를 구분하는 classifier, 즉 hyperplane이 여러개 있을 수 있기 때문에 선택하는 기준이 필요하다.\n",
    "- Margin을 최대화하는 hyperplane이 가장 훌륭한 classifier이고 그를 찾는 방법이다.\n",
    "    + Margin: Support vector와 hyperplane의 거리(support vector와 hyperplane의 수선 거리)\n",
    "    + Support vector: hyperplane과 가장 가까운 데이터들. 양쪽에 하나씩 있으니 최소 2개가 존재하게 된다.\n",
    "- Margin을 최대화하는 이유는 generalization을 좋게하기 때문이다.\n",
    "    + generalization은 학습한 모델이 새로운 데이터에 대해서도 큰 편향 없이 잘 동작한다는 의미다. overfitting의 반대말이라 생각하면 되겠다.(underfitting은 학습 자체가 잘 안된 것)\n",
    "    + 즉 Maximal margin classifier 학습 방식은 모델이 새로운 데이터에도 잘 적용될 수 있도록 학습할 때부터 데이터 편차에 융통성을 주는 방식이다. 여유 혹은 여지를 준다고 봐도 좋다.\n",
    "- 이론을 따라가면 다음 순서대로 구현해야하지만 힘들기 때문에 아래 1.1.2의 방식을 택한다.\n",
    "    + 하나의 hyperplane 지정\n",
    "    + 각 데이터 간의 거리 측정\n",
    "    + Support vector 찾기\n",
    "    + Margin 계산\n",
    "    + 해당 Margin이 다른 hyperplane의 값보다 큰지 비교\n",
    "\n",
    "#### 1.1.2 구현 방식\n",
    "\n",
    "$$\n",
    "\\text{distance} = {|\\ ax + by + c\\ | \\over \\sqrt{a^2 + b^2}}\n",
    "$$\n",
    "\n",
    "- 위의 점과 선의 거리 공식을 이용할 것이다. 이를 활용해 support vector와 hyperplane 간의 거리인 margin을 구한다.\n",
    "- 먼저 위 분자의 $ax + by + c$ 에 절대값을 씌워야한다.\n",
    "    + $ax + by + c$ 는 feature 2개가 있는 데이터 평면을 가정했을 때 hyperplane인 $\\beta_0 + \\beta_1x_1 + \\beta_2x_2$과 같다.\n",
    "    + 이 hyperplane의 결과값이 0인 선을 기준으로 데이터가 분류되게 된다. 즉 양수면 A, 음수면 B이다.\n",
    "    + 계산의 편의를 위해 y 값을 1 또는 -1로 설정한다. logistic regression에서 y값을 0 또는 1로 설정했던 것과 같은 맥락이다.\n",
    "    + $y_i(\\beta_0 + \\beta_1x_1 + \\beta_2x_2) > 0$ 처럼 hyperplane에 y 값을 곱하면 항상 0보다 크게 된다. 즉 절대값을 씌운 것과 같은 맥락이다.\n",
    "- 이 때 $a^2 + b^2$ 값이 1이라면 거리는 분모가 사라져서 margin은 $y_i(\\beta_0 + \\beta_1x_1 + \\beta_2x_2)$ 공식의 값이된다.\n",
    "\n",
    "> 위처럼 $\\beta_1,\\ \\beta_2$의 값이 작게 조정되면 $\\beta_0$의 값이 과하게 커지거나 작아질 수 있다. 하지만 $\\beta_0$의 값은 데이터에 따라서 설정되는 것이기 때문에 나쁜 문제는 아니다.\n",
    "\n",
    "\n",
    "#### 1.1.3 결론\n",
    "\n",
    "$$\n",
    "argmax_{\\beta_0,\\beta_1,\\beta_2} M \\\\\n",
    "\\text{subject to }\\beta_1^2 + \\beta_2^2 = 1\n",
    "$$\n",
    "\n",
    "$a^2 + b^2$ 조건 하에서 margin은 $y_i(\\beta_0 + \\beta_1x_1 + \\beta_2x_2)$ 로 표현되며 이 결과값, 즉 margin을 최대화하는 $\\beta$들의 값을 찾아내는 것이 목표다. 그리고 이 최적화 문제는 **Quadratic program** 방식으로 푼다.\n",
    "\n",
    "### 1.2 Soft Margin Classifier\n",
    "\n",
    "### 1.3 Support Vector Machine\n",
    "\n",
    "## 2. 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
