# KMOOC 인공지능과 기계학습 : W3-1 회귀(regression)

## 1. 지도학습(Supervised Learning)

알고리즘으로 학습하고자하는 대상 함수. 타겟 함수가 있고. input X가 들어오면 Y로 매핑해주는 함수가 타겟 함수다. 알고리즘은 타겟 함수를 학습해야 함. y가 뭐냐에 따라서 분별 태스크, 회귀 태스크로 나뉘어진다.

- 분별: y가 이산 값을 가질 때. 사람의 이미지가 인풋으로 들어왔을 때 남자냐 여자냐.
- 회귀: Y가 continuous한 값일 때.

만약 함수 f가 정해져있다면 학습할 필요가 없다. 즉 함수 f는 모르는 것. 대신 이 함수 f를 나타내는 input, output 데이터값을 모아놨다. 여기서 함수 f를 추정하는 것.

D가 알고리즘의 인풋으로 들어온도록 가정. 해당 인풋이 x일 때 아웃풋이 y였다. 이런 쌍이 n개가 있다는 수식임. 이것을 지도학습이라고 한다. 인풋 x면 그에 해당하는 y값은 뭐야. 라고 딱 데이터셋으로 보여주기 때문에 지도학습이라고 한다.

알고자하는 f에 가장 가깝도록 g를 계속 수정해나간다. 근데 f를 모르는 상황에서 이 g가 f와 비슷한지 어떻게 아는가. 회귀 태스크와 성능 측정법ㅂ.

## 2. 회귀 태스크

타겟 함수 f를 학습하는 것. input -> independent, output -> dependent

f는 모른다. x -> y 쌍으로 이루어진 데이터셋 받을 것. 

이상적으로는 y = f(x) 이지만. 하지만 랜덤 노이즈 입실론에 의해 perturbation(변형)된 것이라고 가정한다. 이 입실론은 평균적으론 노이즈가 없지만(평균0) 정규분포를 따르는 입실론에 의해서 perturbation 될거라고 가정. 노이즈다.

학습 알고리즘(learner): g 함수를 리턴할거다. 희망하기를 함수 f를 최대한 가까이 모사하는 g이기를. 세타 값을 어떻게 세팅하느냐에 따라서 모양이 달라진다.

확률 분포 p(x|y) 첫, 두번째가 가정이다. 인풋 엑스가 들어왔을 때 y의 분포는 정규 분포가 되고. 민은 g 함수의 결과, 그리고 분산은 이전 입실론의 분산과 같다.

log likelihood. 우도를 최대화하는 세타를 찾아가는 과정이다. 계산은 x, y가 동시에 일어날 joint probability를 모든 쌍으로 된 데이터셋 개수만큼 싹 다 곱하는 것. 그리고 그것을 로그를 씌우면 log likelihood다.

우선 각각의 x,y 쌍이 독립적인 것이라 가정하기 때문에 파이 기호를 통해 모든 쌍을 곱할 수 있다.

근데 왜 곱하냐. x, y는 현재 존재하는 데이터 셋이다. x가 input일 때 y가 output인 데이터가 존재하는데 학습 알고리즘으로 나오는 함수 g를 통해 x가 y로 가는지 확률을 계산하는 것. f를 제대로 모사한다는 것은 이 모든 x의 경우에서 g함수가 y로 가야한다는 의미이기 때문에 모든 확률을 곱해야 한다.

product rule을 통해서 x, y의 joint probability를 likelihood와 prior의 곱으로 바꾼다. 그리고 이 곱셈을 로그의 덧셈으로 바꾸면 마지막 식이 나오는데 likelihood의 로그 부분만 세타와 관계있고, 두 번째 p(xi) 부분은 input에 대한 distribution이기 때문에 세타를 조정해도 변함이 없다.

첫 번째 term은 정규분포를 따른다. 랜덤 노이즈와 학습알고리즘을 통해 유추했었다. 그래서 연속함수의 정규분포 PDF 식으로 치환하면 4장의 식이 나온다. 이 식을 앞부분과 뒷 부분(지수 함수) 부분으로 분할 할 수 있다.

- 앞부분은 i와 관련된 것이 없기 때문에 단순히 같은 수가 N번 곱해진 것이다. 그래서 N이 앞으로 나와서 곱해지고, 로그의 분모 분자를 바꾸어 마이너스 값이 된 것
- 뒷부분은 계산을 차근차근 해보면 답이 나온다. `e`의 지수가 대괄호 안의 부분이다. 결국 x, y 값이 1에서 N까지 변하며 e의 제곱수가 곱해지는 것이다. 그렇기 때문에 1에서 N까지 i값이 변하면서 모든 값이 더해지게 된다. 로그가 자연로그이므로 e와 상쇄되어 없어지고 결국 대괄호 안의 지수의 모든 합이 결과가 된다. 공통 부분 상수가 앞으로 나오고 시그마 기호를 통해 표현할 수 있다.

결국 두 번째 항만 중요하게 된다. 첫 번째 항은 세타와 관계 없으므로 무시하고, 두 번째 항에서 시그마 값 또한 상수이므로 무시를 해서 최종적으로 Err 함수가 나오게 된다. 즉 log likelihood를 최대화하는 값은 마이너스가 붙었으므로 Err 값이 최소화 되는 것을 찾으면 된다.

에러 term을 세타 값에 따라서 최소값을 구하면 된다. 즉 데이터에 있는 y값과 학습 알고리즘을 통해 나온 f를 모사하려하는 g 함수 값의 차이를 제곱해서 모두 더한 다음에 2로 나눈 값이다. 이를 통해 나온 세타 값을 least squares estimate라고 한다.

## 3. 선형 회귀

함수 g가 선형 함수라는 가정했을 때 선형 회귀라고 한다.

임의의 선형 함수는 두 개의 파라미터 w1, w2로 임의의 선형 함수를 나타낸다. w1은 xi에 곱해질 계수 coefficient가 되고 w0은 x가 0일 때 y 값을 의미. 즉 `g(x) = w1*xi + w0`이고 세타는 벡터 `{w0, w1}`다.

구하는 방식은 Error term을 w1로 편미분해서 extreme 값을 구하고, 또 w0으로 편미분해서 extreme 값을 구한다. 이 두 개의 방정식으로부터 가장 좋은 w1, w0의 값을 얻어낸다.

g(x)를 대입해서 Err term을 다시 써보면 `Err = 1/2 * sigma1~N (yi - w1xi - w0)^2` 이다.

- w1에 대해서 편미분
    + 제곱이 없어지면서 맨 앞에서 곱해졌던 1/2와 상쇄된다. 계산하면 `sigma1~N (-xi)*(yi- w1xi - w0)` 이 된다. 김기응 교수님은 편미분할 때 자연스럽게 w1의 계수 -xi를 바깥으로 묶어냈는데 무슨 규칙이 있나보다. 이 부분을 잘 몰라서 제곱 식을 계산해서 편미분해봤더니 동일한 결과가 나왔다.
    + 그리고 이 식을 0으로 세팅해서(extreme 값이 되는) w1을 구하고싶은거다. `sigma(xiyi)`를 우변으로 넘겨서 좌변을 `w0*sigma(xi) + w1*sigma(xi^2)` 꼴로 변경한다.
- w0에 대해서 편미분
    + 제곱이 없어지면서 앞의 1/2와 상쇄되고 w0의 계수인 -1이 뒤에 곱해진다. 이것을 0으로 세팅해서 정리하면 `w1*sigma(xi) + N*w0 = sigma(yi)`

## 다변량 회귀(Multivariate Regression)


















